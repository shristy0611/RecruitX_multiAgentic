from sqlalchemy.orm import Session
from typing import List, Optional, Dict, Any
import logging

from recruitx_app.models.candidate import Candidate
from recruitx_app.agents.cv_analysis_agent import CVAnalysisAgent
from recruitx_app.services.vector_db_service import vector_db_service
from recruitx_app.utils.text_utils import split_text
from recruitx_app.schemas.candidate import CandidateAnalysis

logger = logging.getLogger(__name__)

class CandidateService:
    """
    Service for handling candidate-related operations.
    """
    
    def __init__(self):
        """Initialize the candidate service with the CV analysis agent."""
        self.cv_agent = CVAnalysisAgent()
    
    def get_candidates(self, db: Session, skip: int = 0, limit: int = 100) -> List[Candidate]:
        """Get a list of candidates with pagination."""
        return db.query(Candidate).offset(skip).limit(limit).all()
    
    def get_candidate(self, db: Session, candidate_id: int) -> Optional[Candidate]:
        """Get a specific candidate by ID."""
        return db.query(Candidate).filter(Candidate.id == candidate_id).first()
    
    def create_candidate(self, db: Session, candidate_data: Dict[str, Any]) -> Candidate:
        """
        Create a new candidate.
        
        Args:
            db: Database session
            candidate_data: Dictionary containing candidate information
            
        Returns:
            The created candidate
        """
        db_candidate = Candidate(**candidate_data)
        db.add(db_candidate)
        db.commit()
        db.refresh(db_candidate)
        return db_candidate
    
    async def analyze_cv(self, db: Session, candidate_id: int) -> Optional[CandidateAnalysis]:
        """
        Analyze a candidate's CV using the agent, update the database, and index the content.
        Returns the CandidateAnalysis object upon success, otherwise None.
        """
        candidate = self.get_candidate(db, candidate_id)
        if not candidate or not candidate.resume_raw:
            logger.warning(f"Candidate {candidate_id} not found or has no resume text for analysis.")
            return None
            
        # Step 1: Analyze the CV using the agent (now expects candidate_id)
        logger.info(f"Starting CV analysis for candidate ID: {candidate_id}")
        # Agent now returns CandidateAnalysis object or None
        analysis_result: Optional[CandidateAnalysis] = await self.cv_agent.analyze_cv(
            cv_text=candidate.resume_raw, 
            candidate_id=candidate_id
        )
        
        if not analysis_result:
            logger.error(f"CV analysis failed for candidate ID: {candidate_id}. Agent returned None.")
            # Optionally update candidate status in DB to reflect analysis failure?
            return None

        # Step 2: Update the candidate record with analysis results (using model_dump)
        try:
            analysis_dict = analysis_result.model_dump(mode='json') # Convert Pydantic model to dict for JSON storage
            candidate.analysis = analysis_dict 
            db.commit()
            db.refresh(candidate)
            logger.info(f"Successfully updated candidate {candidate_id} with analysis results.")
        except Exception as e:
            db.rollback()
            logger.error(f"Failed to save analysis results for candidate {candidate_id}: {e}", exc_info=True)
            # If saving fails, should we still index? Let's return None to indicate overall failure.
            return None
            
        # Step 3: Chunk and index the raw resume text for RAG (no change needed here)
        try:
            logger.info(f"Starting chunking and indexing for candidate ID: {candidate_id}")
            chunks = split_text(candidate.resume_raw)
            
            if not chunks:
                logger.warning(f"No text chunks generated by split_text for candidate ID: {candidate_id}")
                # Even if indexing fails, the analysis was successful, so return the result.
                return analysis_result 
                
            metadatas = []
            ids = []
            for i, chunk in enumerate(chunks):
                doc_id = f"cand_{candidate_id}"
                chunk_id = f"{doc_id}_chunk_{i}"
                metadatas.append({
                    "doc_id": doc_id,
                    "doc_type": "candidate",
                    "candidate_id": candidate_id,
                    "chunk_index": i
                })
                ids.append(chunk_id)
            
            logger.info(f"Attempting to index {len(chunks)} chunks for candidate ID: {candidate_id}")
            success = await vector_db_service.add_document_chunks(
                documents=chunks,
                metadatas=metadatas,
                ids=ids
            )
            
            if success:
                logger.info(f"Successfully indexed {len(chunks)} chunks for candidate ID: {candidate_id}")
            else:
                logger.error(f"Vector DB service reported failure indexing chunks for candidate ID: {candidate_id}")
                # Log error but still return the successful analysis result.

        except Exception as e:
             logger.error(f"Error during chunking/indexing for candidate {candidate_id}: {e}", exc_info=True)
             # Log error but still return the successful analysis result.
            
        # Return the validated CandidateAnalysis object if everything up to DB save succeeded.
        return analysis_result